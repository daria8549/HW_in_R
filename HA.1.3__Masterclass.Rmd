---
title: "IC.1.3 Masterclass"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# HW: Homework exercises

When submitting to Canvas, make sure to submit this .Rmd file with your solutions and not the knitted PDF, HTML or any other file. Note: In order to be able to complete the assignments, you may need to re-run the .Rmd codeblocks in the current week's notebook to have the variables required stored in your local environment.

## HW.3 (Masterclass - 1 point)

We have seen that testing different recommender models and approaches present a wide-range of challenges when comparing and evaluating their effectiveness. Your task now is to compare three models using a metric other than the F1-score or runtime.
Choose three models from Unit 1 and evaluate these models with the new metric. Explain in a couple of sentences how this new evaluation metric can aid our efforts in searching for a superior recommendation algorithm and what it means for the bigger picture concerning LastCentury.

```{r}
library(recommenderlab)

rating_matrix <- as(ML_df, "realRatingMatrix")

# training models
ubcf_model <- Recommender(rating_matrix, method = "UBCF")
ibcf_model <- Recommender(rating_matrix, method = "IBCF")
svd_model <- Recommender(rating_matrix, method = "SVD")

# predictions
ubcf_predicted <- predict(ubcf_model, rating_matrix, type="ratings")
ibcf_predicted <- predict(ibcf_model, rating_matrix, type="ratings")
svd_predicted <- predict(svd_model, rating_matrix, type="ratings")

# extracting actual ratings from the dataset
actual_ratings <- rating_matrix@data

# Mean Absolute Error
mae <- function(predicted, actual) {
  mean(abs(predicted - actual), na.rm = TRUE)
}

# predictions into matrix format
ubcf_matrix <- as(ubcf_predicted, "matrix")
ibcf_matrix <- as(ibcf_predicted, "matrix")
svd_matrix <- as(svd_predicted, "matrix")

# MAE
ubcf_mae <- mae(as.numeric(ubcf_matrix), as.numeric(actual_ratings))
ibcf_mae <- mae(as.numeric(ibcf_matrix), as.numeric(actual_ratings))
svd_mae <- mae(as.numeric(svd_matrix), as.numeric(actual_ratings))


all_evaluations <- rbind(UBCF=data.frame(MAE=ubcf_mae),
                         IBCF=data.frame(MAE=ibcf_mae),
                         SVD=data.frame(MAE=svd_mae))

print(all_evaluations)
```


```

MAE shows how far is prediction from actual user preference and compeared to F1 score focuses on accuracy of generated prediction instead of classification into 'good' or 'bad'. Which might be more relevant for personilized recomendation system.

```

